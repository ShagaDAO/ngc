# The Shaga Network: Gaming Infrastructure That Learns

**The operational infrastructure built by Shaga Labs, run by Shaga users, generating data for Neural Game Codecs research.**

## What It Is

The Shaga Network connects gaming PCs to create a distributed system that serves real-time gaming while capturing data for AI. It currently powers 165K monthly users with low-latency streaming, turning idle hardware into a self-sustaining ecosystem.

**The Vision**: Transform this infrastructure into the world's first Neural Content Delivery Network, using AI to compress causality (controller inputs + visuals) for minimal bandwidth and enhanced experiences.

## Current Network Stats (July 2025)

| Category | Metrics |
|----------|---------|
| **Operational** | • 608 active nodes serving users globally<br>• 165K monthly users streaming games<br>• 200K+ hours of game streaming delivered<br>• 39.51 Gbps total network throughput |
| **Compute & Hardware** | • 4.19 PFLOPS total GPU compute (FP32)<br>• 848 nodes analyzed (333 with full profiling)<br>• 2.97 TB total RAM<br>• 72.4 TB total storage |
| **Addressable Market** | • 60 million node-ready gaming PCs globally<br>• Potential expansion: 100,000x current scale<br>• Idle opportunity: 1.2 billion GPU-hours wasted daily |

**Source**: Shaga Explorer v2025-06 (cross-verified with mid-2025 reports showing similar growth, e.g., 74+ supported GPU models).

## How It Works

Users connect gaming PCs to serve others via P2P mesh. Each session:
- Burns 200-500W to generate ~60 FPS visuals
- Creates synchronized controller input + output pairs
- Delivers real games with independent, scalable connections

This generates billions of causality pairs across diverse hardware and engines—untapped training data for neural codecs.

## The Economics

**Token Model**: Ties emissions to verifiable hardware for "Proof of Useful Work."
```
Token_Emission = (Hardware_TDP × Local_electricity_rate) × 1.1
```

**Key Features**:
- Requires real deployment (validated via GPU profiling and challenges)
- Thin 10% margins deter farming
- Geographic accuracy reflects infrastructure costs

Peak hours drive revenue; off-peak enables AI training for long-term value (e.g., bandwidth savings boosting retention).

## Network Architecture

**Distributed Data Collection**: P2P with crypto incentives, capturing gameplay across hardware for neural training.

**Decentralized Compute**: Idle PCs handle model training/inference, with geographic distribution ensuring <35ms latency vs. centralized clouds.

## The Research Connection

The network feeds [Neural Game Codecs](../README.md) research by providing:
- Synchronized inputs + visuals for causality compression
- Game-specific patterns under real loads
- Distributed compute for training

When codecs mature, this evolves the network into a Neural Content Delivery Network—AI-optimized for bandwidth reduction and interactive media.
